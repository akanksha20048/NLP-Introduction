{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from re import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    filterdText=tokenizer.tokenize(text)\n",
    "    print(\"Number of words:\",len(filterdText))\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    print(\"Number of sentences:\",len(sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 314\n",
      "Number of sentences: 16\n"
     ]
    }
   ],
   "source": [
    "read_file('101725')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file2(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    r = compile(r'(\\b[aeiouAEIOU][a-zA-Z]*[0-9]*)', MULTILINE)\n",
    "    print(\"Number of Words Starting with vowels:\",len(r.findall(text)))\n",
    "    r = compile(r'\\b[b-df-hj-np-tv-zB-DF-HJ-NP-TV-Z][a-zA-Z]*[0-9]*', MULTILINE)\n",
    "    print(\"Number of words starting with consonants:\",len(r.findall(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words Starting with vowels: 76\n",
      "Number of words starting with consonants: 218\n"
     ]
    }
   ],
   "source": [
    "read_file2('101725')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file3(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    r = compile(r'\\S+@\\S+[.]\\S+', MULTILINE)\n",
    "    print(\"email-ids:\",r.findall(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email-ids: ['<1993Mar22.215141.28352@mri.com>', 'jeff@mri.com', 'jeff@mir.com', '<C41soE.M62@ns1.nodak.edu>', '<C41soE.M62@ns1.nodak.edu>', 'wilken@plains.NoDak.edu', 'wilken@plains.nodak.edu', 'jeff@mri.com']\n"
     ]
    }
   ],
   "source": [
    "read_file3('101725')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file4(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    text = text.lower()\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    tokenizer = sent_tokenize(text)\n",
    "    word = input(\"enter the first word of sentence : \")\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    for w in tokenizer:\n",
    "        if re.findall('^'+word+\" \",w):\n",
    "            print(w)\n",
    "            count = count+1\n",
    "    print(\" \")\n",
    "    print(\"Count is: \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the first word of sentence : telephone\n",
      "telephone in the bay area is 415-962-8430.\n",
      "i'm not sure how amenable they are to shipping.\n",
      " \n",
      "Count is:  1\n"
     ]
    }
   ],
   "source": [
    "read_file4('101725')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file5(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    text = text.lower()\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    tokenizer = sent_tokenize(text)\n",
    "    word = input(\"enter the last word of sentence : \")\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    i = 1\n",
    "    for w in tokenizer:\n",
    "        if re.findall(\" \"+word+\".?$\",w):\n",
    "            print(w)\n",
    "            print(\" \")\n",
    "            count = count+1\n",
    "    print(\" \")\n",
    "    print(\"Count is: \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the last word of sentence : polish\n",
      "path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!husc-news.harvard.edu!kuhub.cc.ukans.edu!hubble.asymetrix.com!rpi!uwm.edu!biosci!parc!decwrl!decwrl!uunet!mri!jeff\n",
      "newsgroups: rec.motorcycles\n",
      "subject: re: lexan polish?\n",
      " \n",
      "|     uucp: ..!uunet!plains!wilken      |     dod #0087 \n",
      ">vf700f interceptor  |        bitnet:  wilken@plains         |   \n",
      "\n",
      "suggest mcquires #1 plastic polish.\n",
      " \n",
      " \n",
      "Count is:  2\n"
     ]
    }
   ],
   "source": [
    "read_file5('101725')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file6(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    text = text.lower()\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    tokenizer = sent_tokenize(text)\n",
    "    word = input(\"enter the word to be searched : \")\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    i = 1\n",
    "    for w in tokenizer:\n",
    "        if re.findall(word,w):\n",
    "            print(\" \")\n",
    "            count = count + 1\n",
    "            print(i,\" sentence which has: \",word, \" is: \")\n",
    "            print(w)\n",
    "            print(\"______________________________________________________________\")\n",
    "            i = i+1\n",
    "    print(\" \")\n",
    "    print(\"Total count is: \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the word to be searched : polish\n",
      " \n",
      "1  sentence which has:  polish  is: \n",
      "path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!husc-news.harvard.edu!kuhub.cc.ukans.edu!hubble.asymetrix.com!rpi!uwm.edu!biosci!parc!decwrl!decwrl!uunet!mri!jeff\n",
      "newsgroups: rec.motorcycles\n",
      "subject: re: lexan polish?\n",
      "______________________________________________________________\n",
      " \n",
      "2  sentence which has:  polish  is: \n",
      ">\n",
      ">can anyone recommend a polish to use on it that is safe for lexan?\n",
      "______________________________________________________________\n",
      " \n",
      "3  sentence which has:  polish  is: \n",
      "its\n",
      ">starting to show a few scratches, and id like to polish them out..\n",
      ">go fast!\n",
      "______________________________________________________________\n",
      " \n",
      "4  sentence which has:  polish  is: \n",
      "|     uucp: ..!uunet!plains!wilken      |     dod #0087 \n",
      ">vf700f interceptor  |        bitnet:  wilken@plains         |   \n",
      "\n",
      "suggest mcquires #1 plastic polish.\n",
      "______________________________________________________________\n",
      " \n",
      "Total count is:  4\n"
     ]
    }
   ],
   "source": [
    "read_file6('101725')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file7(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    r = compile(r'[a-z A-Z]*[0-9]*\\?', MULTILINE)\n",
    "    print(r.findall(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Lexan Polish?', 'Can anyone recommend a polish to use on it that is safe for lexan?']\n"
     ]
    }
   ],
   "source": [
    "read_file7('101725')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file8(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    r = compile(r'([012]*[0-9]) [a-zA-Z]* [0-9][0-9][0-9]*[0-9]* (2[0-3]|[01][0-9]):([0-5][0-9]):([0-5]?[0-9])', MULTILINE) \n",
    "    time = r.findall(text)\n",
    "    for t in range (len(time)): \n",
    "        for ms in range (len(time[t])):\n",
    "            if ms == 0:\n",
    "                continue;\n",
    "            if ms == 2:\n",
    "                print(time[t][ms],\"minutes\", end = \" \")\n",
    "            if ms == 3:\n",
    "                print(time[t][ms],\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 minutes 41 seconds\n",
      "00 minutes 00 seconds\n"
     ]
    }
   ],
   "source": [
    "read_file8('101725')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file9(file):\n",
    "    fp = codecs.open(file, 'r', encoding = 'utf-8', errors = 'ignore')\n",
    "    text = fp.read()\n",
    "    r = compile(r'(\\b(?:[A-Z][a-z]*){2,})', MULTILINE)\n",
    "    print(r.findall(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'GMT', 'GMT', 'USA', 'NoDak', 'FAST', 'AMA', 'UUCP', 'DoD', 'VF', 'WILKEN', 'PLAINS', 'McQuires', 'McQuires', 'TAP', 'PLASTIC']\n"
     ]
    }
   ],
   "source": [
    "read_file9('101725')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
